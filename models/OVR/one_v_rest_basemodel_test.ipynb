{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "import keras\n",
    "import tensorflow\n",
    "import tensorflow_addons as tfa\n",
    "from keras import layers, Model\n",
    "from keras.models import Sequential\n",
    "from keras.applications import DenseNet201\n",
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = './UrbanSound8K/metadata/UrbanSound8K.csv'\n",
    "spec_path = \"./numpySpectrograms/\"\n",
    "model_save_path = \"ovr_basemodel\"\n",
    "test_size = 0.2\n",
    "val_size = 0.2\n",
    "batch_size = 16\n",
    "num_classes = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpecLoader(keras.utils.Sequence):\n",
    "  def __init__(self, x_set, y_set, batch_size, spec_dir):\n",
    "    self.x, self.y = x_set, y_set\n",
    "    self.batch_size = batch_size\n",
    "    self.spec_dir = spec_dir\n",
    "\n",
    "  def __len__(self):\n",
    "    return int(np.ceil(len(self.x) / self.batch_size))\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    batch_x = self.x[idx * self.batch_size:(idx + 1) *\n",
    "    self.batch_size]\n",
    "    batch_y = self.y[idx * self.batch_size:(idx + 1) *\n",
    "        self.batch_size]\n",
    "\n",
    "    batchSpecs = []\n",
    "    for fileName in batch_x:\n",
    "        spec = np.load(self.spec_dir + fileName + \".npy\")\n",
    "        batchSpecs.append(spec.transpose())\n",
    "    return np.array(batchSpecs), np.array(batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(csv_path)\n",
    "data_df_known = data_df.loc[data_df[\"classID\"] < num_classes]\n",
    "data_df_unknown = data_df.loc[data_df[\"classID\"] >= num_classes]\n",
    "\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(data_df_known['slice_file_name'].tolist(), data_df_known['classID'].tolist(), test_size=test_size, random_state = 42)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=val_size, random_state = 42)\n",
    "X_trash, X_unknown, y_trash, y_unknown = train_test_split(data_df_unknown['slice_file_name'].tolist(), data_df_unknown['classID'].tolist(), test_size=test_size, random_state = 42)\n",
    "y_unknown = [num_classes] * len(y_unknown)\n",
    "\n",
    "X_test = X_test + X_unknown\n",
    "y_test = y_test + y_unknown\n",
    "test_loader = SpecLoader(X_test, y_test, batch_size, spec_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "models = []\n",
    "for i in range(num_classes):\n",
    "    models.append(keras.models.load_model(model_save_path+str(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(predicted, expected):\n",
    "    acc = np.mean(np.array(predicted) == np.array(expected))\n",
    "    print(\"Overall accuracy: {}\".format(acc))\n",
    "    acc_dict = {}\n",
    "    for i in range(len(expected)):\n",
    "        expected_class = expected[i]\n",
    "        if expected_class not in acc_dict:\n",
    "            acc_dict[expected_class] = [0, 0]\n",
    "        acc_dict[expected_class][1] += 1\n",
    "        if expected_class == predicted[i]:\n",
    "            acc_dict[expected_class][0] += 1\n",
    "    for k,v in acc_dict.items():\n",
    "        print(\"Accuracy for class {}: {}\".format(k, v[0]/v[1]))\n",
    "    for average in [\"macro\", \"weighted\", \"micro\"]:\n",
    "        f1 = f1_score(expected, predicted, average=average)\n",
    "        print(\"{} f1 score: {}\".format(average, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [07:09<00:00,  3.91s/it]\n"
     ]
    }
   ],
   "source": [
    "y_predicted = []\n",
    "y_random = []\n",
    "for data, labels in tqdm(test_loader):\n",
    "    for i in range(len(data)):\n",
    "        probabilities = []\n",
    "        for j in range(num_classes):\n",
    "            probabilities.append(models[j].predict(x=np.array([data[i]]))[0][1])\n",
    "        best_class = np.argmax(probabilities)\n",
    "        highest_prob = probabilities[best_class]\n",
    "        if highest_prob < 0.5:\n",
    "            y_predicted.append(num_classes)\n",
    "        else:\n",
    "            y_predicted.append(best_class)\n",
    "        y_random.append(np.random.randint(num_classes + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy: 0.6119061247853463\n",
      "Accuracy for class 2: 0.8669950738916257\n",
      "Accuracy for class 4: 0.9418604651162791\n",
      "Accuracy for class 0: 0.8516746411483254\n",
      "Accuracy for class 3: 0.7807017543859649\n",
      "Accuracy for class 1: 0.972972972972973\n",
      "Accuracy for class 5: 0.3519163763066202\n",
      "macro f1 score: 0.6698090056903326\n",
      "weighted f1 score: 0.5987305771258042\n",
      "micro f1 score: 0.6119061247853463\n",
      "Overall accuracy: 0.16256439610761306\n",
      "Accuracy for class 2: 0.17733990147783252\n",
      "Accuracy for class 4: 0.13953488372093023\n",
      "Accuracy for class 0: 0.16267942583732056\n",
      "Accuracy for class 3: 0.16666666666666666\n",
      "Accuracy for class 1: 0.1891891891891892\n",
      "Accuracy for class 5: 0.1602787456445993\n",
      "macro f1 score: 0.1411546645788542\n",
      "weighted f1 score: 0.18388591180544697\n",
      "micro f1 score: 0.16256439610761306\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_predicted, y_test)\n",
    "evaluate(y_random, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
